{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-25 22:55:53--  https://kdd.ics.uci.edu/databases/reuters_transcribed/ReutersTranscribedSubset.zip\n",
      "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
      "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 148545 (145K) [application/zip]\n",
      "Saving to: ‘ReutersTranscribedSubset.zip.5’\n",
      "\n",
      "ReutersTranscribedS 100%[===================>] 145.06K   129KB/s    in 1.1s    \n",
      "\n",
      "2018-04-25 22:55:56 (129 KB/s) - ‘ReutersTranscribedSubset.zip.5’ saved [148545/148545]\n",
      "\n",
      "bigdata\t\t\t      ReutersTranscribedSubset.zip.1\n",
      "data\t\t\t      ReutersTranscribedSubset.zip.2\n",
      "derby.log\t\t      ReutersTranscribedSubset.zip.3\n",
      "lab9-streaming_1\t      ReutersTranscribedSubset.zip.4\n",
      "metastore_db\t\t      ReutersTranscribedSubset.zip.5\n",
      "pixiedust.db\t\t      transcriptions\n",
      "ReutersTranscribed\t      Untitled2.ipynb\n",
      "ReutersTranscribedSubset.zip  Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "## Obtaining zip file data from online directory.\n",
    "!wget https://kdd.ics.uci.edu/databases/reuters_transcribed/ReutersTranscribedSubset.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ReutersTranscribedSubset.zip\n",
      "  inflating: transcriptions/acq/14843  \n",
      "  inflating: transcriptions/acq/14852  \n",
      "  inflating: transcriptions/acq/14865  \n",
      "  inflating: transcriptions/acq/14888  \n",
      "  inflating: transcriptions/acq/14900  \n",
      "  inflating: transcriptions/acq/14907  \n",
      "  inflating: transcriptions/acq/14909  \n",
      "  inflating: transcriptions/acq/14921  \n",
      "  inflating: transcriptions/acq/14932  \n",
      "  inflating: transcriptions/acq/14941  \n",
      "  inflating: transcriptions/acq/14943  \n",
      "  inflating: transcriptions/acq/14949  \n",
      "  inflating: transcriptions/acq/14978  \n",
      "  inflating: transcriptions/acq/14982  \n",
      "  inflating: transcriptions/acq/15001  \n",
      "  inflating: transcriptions/acq/15004  \n",
      "  inflating: transcriptions/acq/15024  \n",
      "  inflating: transcriptions/acq/15031  \n",
      "  inflating: transcriptions/acq/15037  \n",
      "  inflating: transcriptions/acq/15052  \n",
      "  inflating: transcriptions/corn/14832  \n",
      "  inflating: transcriptions/corn/14858  \n",
      "  inflating: transcriptions/corn/15033  \n",
      "  inflating: transcriptions/corn/15043  \n",
      "  inflating: transcriptions/corn/15106  \n",
      "  inflating: transcriptions/corn/15287  \n",
      "  inflating: transcriptions/corn/15341  \n",
      "  inflating: transcriptions/corn/15618  \n",
      "  inflating: transcriptions/corn/15648  \n",
      "  inflating: transcriptions/corn/15676  \n",
      "  inflating: transcriptions/corn/15686  \n",
      "  inflating: transcriptions/corn/15720  \n",
      "  inflating: transcriptions/corn/15845  \n",
      "  inflating: transcriptions/corn/15856  \n",
      "  inflating: transcriptions/corn/15860  \n",
      "  inflating: transcriptions/corn/15863  \n",
      "  inflating: transcriptions/corn/15871  \n",
      "  inflating: transcriptions/corn/15875  \n",
      "  inflating: transcriptions/corn/15877  \n",
      "  inflating: transcriptions/corn/15890  \n",
      "  inflating: transcriptions/crude/14829  \n",
      "  inflating: transcriptions/crude/15063  \n",
      "  inflating: transcriptions/crude/15200  \n",
      "  inflating: transcriptions/crude/15230  \n",
      "  inflating: transcriptions/crude/15238  \n",
      "  inflating: transcriptions/crude/15244  \n",
      "  inflating: transcriptions/crude/15322  \n",
      "  inflating: transcriptions/crude/15339  \n",
      "  inflating: transcriptions/crude/15344  \n",
      "  inflating: transcriptions/crude/15351  \n",
      "  inflating: transcriptions/crude/15520  \n",
      "  inflating: transcriptions/crude/15939  \n",
      "  inflating: transcriptions/crude/15964  \n",
      "  inflating: transcriptions/crude/16005  \n",
      "  inflating: transcriptions/crude/16007  \n",
      "  inflating: transcriptions/crude/16040  \n",
      "  inflating: transcriptions/crude/16077  \n",
      "  inflating: transcriptions/crude/16366  \n",
      "  inflating: transcriptions/crude/16429  \n",
      "  inflating: transcriptions/crude/16438  \n",
      "  inflating: transcriptions/earn/14859.txt  \n",
      "  inflating: transcriptions/earn/14860.txt  \n",
      "  inflating: transcriptions/earn/14872.txt  \n",
      "  inflating: transcriptions/earn/14873.txt  \n",
      "  inflating: transcriptions/earn/14875.txt  \n",
      "  inflating: transcriptions/earn/14876.txt  \n",
      "  inflating: transcriptions/earn/14899.txt  \n",
      "  inflating: transcriptions/earn/14903.txt  \n",
      "  inflating: transcriptions/earn/14911.txt  \n",
      "  inflating: transcriptions/earn/14926.txt  \n",
      "  inflating: transcriptions/earn/14930.txt  \n",
      "  inflating: transcriptions/earn/14933.txt  \n",
      "  inflating: transcriptions/earn/14934.txt  \n",
      "  inflating: transcriptions/earn/14954.txt  \n",
      "  inflating: transcriptions/earn/14958.txt  \n",
      "  inflating: transcriptions/earn/14960.txt  \n",
      "  inflating: transcriptions/earn/14962.txt  \n",
      "  inflating: transcriptions/earn/14963.txt  \n",
      "  inflating: transcriptions/earn/14965.txt  \n",
      "  inflating: transcriptions/earn/14967.txt  \n",
      "  inflating: transcriptions/grain/14828.txt  \n",
      "  inflating: transcriptions/grain/14832.txt  \n",
      "  inflating: transcriptions/grain/14841.txt  \n",
      "  inflating: transcriptions/grain/14858.txt  \n",
      "  inflating: transcriptions/grain/15033.txt  \n",
      "  inflating: transcriptions/grain/15043.txt  \n",
      "  inflating: transcriptions/grain/15097.txt  \n",
      "  inflating: transcriptions/grain/15106.txt  \n",
      "  inflating: transcriptions/grain/15132.txt  \n",
      "  inflating: transcriptions/grain/15271.txt  \n",
      "  inflating: transcriptions/grain/15273.txt  \n",
      "  inflating: transcriptions/grain/15287.txt  \n",
      "  inflating: transcriptions/grain/15303.txt  \n",
      "  inflating: transcriptions/grain/15341.txt  \n",
      "  inflating: transcriptions/grain/15367.txt  \n",
      "  inflating: transcriptions/grain/15388.txt  \n",
      "  inflating: transcriptions/grain/15472.txt  \n",
      "  inflating: transcriptions/grain/15500.txt  \n",
      "  inflating: transcriptions/grain/15531.txt  \n",
      "  inflating: transcriptions/grain/15567.txt  \n",
      "  inflating: transcriptions/interest/14849.txt  \n",
      "  inflating: transcriptions/interest/14861.txt  \n",
      "  inflating: transcriptions/interest/14890.txt  \n",
      "  inflating: transcriptions/interest/14919.txt  \n",
      "  inflating: transcriptions/interest/14964.txt  \n",
      "  inflating: transcriptions/interest/15049.txt  \n",
      "  inflating: transcriptions/interest/15092.txt  \n",
      "  inflating: transcriptions/interest/15096.txt  \n",
      "  inflating: transcriptions/interest/15212.txt  \n",
      "  inflating: transcriptions/interest/15310.txt  \n",
      "  inflating: transcriptions/interest/15364.txt  \n",
      "  inflating: transcriptions/interest/15378.txt  \n",
      "  inflating: transcriptions/interest/15384.txt  \n",
      "  inflating: transcriptions/interest/15410.txt  \n",
      "  inflating: transcriptions/interest/15436.txt  \n",
      "  inflating: transcriptions/interest/15444.txt  \n",
      "  inflating: transcriptions/interest/15522.txt  \n",
      "  inflating: transcriptions/interest/15539.txt  \n",
      "  inflating: transcriptions/interest/15550.txt  \n",
      "  inflating: transcriptions/interest/15552.txt  \n",
      "  inflating: transcriptions/money/14849  \n",
      "  inflating: transcriptions/money/14861  \n",
      "  inflating: transcriptions/money/14890  \n",
      "  inflating: transcriptions/money/14913  \n",
      "  inflating: transcriptions/money/14919  \n",
      "  inflating: transcriptions/money/14931  \n",
      "  inflating: transcriptions/money/14964  \n",
      "  inflating: transcriptions/money/14987  \n",
      "  inflating: transcriptions/money/15048  \n",
      "  inflating: transcriptions/money/15212  \n",
      "  inflating: transcriptions/money/15234  \n",
      "  inflating: transcriptions/money/15253  \n",
      "  inflating: transcriptions/money/15364  \n",
      "  inflating: transcriptions/money/15375  \n",
      "  inflating: transcriptions/money/15378  \n",
      "  inflating: transcriptions/money/15431  \n",
      "  inflating: transcriptions/money/15436  \n",
      "  inflating: transcriptions/money/15442  \n",
      "  inflating: transcriptions/money/15444  \n",
      "  inflating: transcriptions/money/15448  \n",
      "  inflating: transcriptions/ship/14839.txt  \n",
      "  inflating: transcriptions/ship/14957.txt  \n",
      "  inflating: transcriptions/ship/14959.txt  \n",
      "  inflating: transcriptions/ship/15484.txt  \n",
      "  inflating: transcriptions/ship/15531.txt  \n",
      "  inflating: transcriptions/ship/15696.txt  \n",
      "  inflating: transcriptions/ship/15710.txt  \n",
      "  inflating: transcriptions/ship/15726.txt  \n",
      "  inflating: transcriptions/ship/15727.txt  \n",
      "  inflating: transcriptions/ship/15728.txt  \n",
      "  inflating: transcriptions/ship/15942.txt  \n",
      "  inflating: transcriptions/ship/16014.txt  \n",
      "  inflating: transcriptions/ship/16040.txt  \n",
      "  inflating: transcriptions/ship/16076.txt  \n",
      "  inflating: transcriptions/ship/16366.txt  \n",
      "  inflating: transcriptions/ship/16934.txt  \n",
      "  inflating: transcriptions/ship/17055.txt  \n",
      "  inflating: transcriptions/ship/17192.txt  \n",
      "  inflating: transcriptions/ship/17436.txt  \n",
      "  inflating: transcriptions/ship/17462.txt  \n",
      "  inflating: transcriptions/trade/14826.txt  \n",
      "  inflating: transcriptions/trade/14832.txt  \n",
      "  inflating: transcriptions/trade/14858.txt  \n",
      "  inflating: transcriptions/trade/14862.txt  \n",
      "  inflating: transcriptions/trade/14881.txt  \n",
      "  inflating: transcriptions/trade/14904.txt  \n",
      "  inflating: transcriptions/trade/14912.txt  \n",
      "  inflating: transcriptions/trade/15154.txt  \n",
      "  inflating: transcriptions/trade/15171.txt  \n",
      "  inflating: transcriptions/trade/15223.txt  \n",
      "  inflating: transcriptions/trade/15262.txt  \n",
      "  inflating: transcriptions/trade/15313.txt  \n",
      "  inflating: transcriptions/trade/15352.txt  \n",
      "  inflating: transcriptions/trade/15372.txt  \n",
      "  inflating: transcriptions/trade/15375.txt  \n",
      "  inflating: transcriptions/trade/15386.txt  \n",
      "  inflating: transcriptions/trade/15428.txt  \n",
      "  inflating: transcriptions/trade/15430.txt  \n",
      "  inflating: transcriptions/trade/15447.txt  \n",
      "  inflating: transcriptions/trade/15452.txt  \n",
      "  inflating: transcriptions/trade/15552.txt  \n",
      "  inflating: transcriptions/wheat/14841.txt  \n",
      "  inflating: transcriptions/wheat/15043.txt  \n",
      "  inflating: transcriptions/wheat/15097.txt  \n",
      "  inflating: transcriptions/wheat/15132.txt  \n",
      "  inflating: transcriptions/wheat/15271.txt  \n",
      "  inflating: transcriptions/wheat/15273.txt  \n",
      "  inflating: transcriptions/wheat/15341.txt  \n",
      "  inflating: transcriptions/wheat/15388.txt  \n",
      "  inflating: transcriptions/wheat/15472.txt  \n",
      "  inflating: transcriptions/wheat/15500.txt  \n",
      "  inflating: transcriptions/wheat/15567.txt  \n",
      "  inflating: transcriptions/wheat/15572.txt  \n",
      "  inflating: transcriptions/wheat/15582.txt  \n",
      "  inflating: transcriptions/wheat/15618.txt  \n",
      "  inflating: transcriptions/wheat/15676.txt  \n",
      "  inflating: transcriptions/wheat/15686.txt  \n",
      "  inflating: transcriptions/wheat/15728.txt  \n",
      "  inflating: transcriptions/wheat/15836.txt  \n",
      "  inflating: transcriptions/wheat/15845.txt  \n",
      "  inflating: transcriptions/wheat/15853.txt  \n",
      "bigdata\t\t\t      ReutersTranscribedSubset.zip.1\n",
      "data\t\t\t      ReutersTranscribedSubset.zip.2\n",
      "derby.log\t\t      ReutersTranscribedSubset.zip.3\n",
      "lab9-streaming_1\t      ReutersTranscribedSubset.zip.4\n",
      "metastore_db\t\t      ReutersTranscribedSubset.zip.5\n",
      "pixiedust.db\t\t      transcriptions\n",
      "ReutersTranscribed\t      Untitled2.ipynb\n",
      "ReutersTranscribedSubset.zip  Untitled.ipynb\n",
      ">>> Unzipping finished.\n"
     ]
    }
   ],
   "source": [
    "## Unzipping the data zip file.\n",
    "!unzip -o ReutersTranscribedSubset.zip\n",
    "\n",
    "# '!' calls a program on the machine (the DSX service runs on virtual Linux machines).\n",
    "!ls\n",
    "print(\">>> Unzipping finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p:  /home/enterprise.internal.city.ac.uk/acvn747/transcriptions\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/acq/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/corn/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/crude/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/earn/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/grain/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/interest/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/money/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/ship/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/trade/\n",
      "/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/wheat/\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/acq/14843', \"\\r\\n... Soon with the mood and insert quick recovery from much soon is certain lose its status as chip warns most profitable bank cuts result of its merger with the high were so woman financial analysis said osaka based sony to move with ... Deposits of a run print three pun nine trillion yen emerged with i were so ago ... A small struggling pant with tennis timid in one pen to nine billion dollars enough in un recovery billions ... Knocked over ... When despite the link talk soon with some of president co kumar sue told reuters is confident his banking quickly regain its position ... Will be back ... In position in first place within three years comma so said none intervened he said that while the merger will find will initially reduce omit the most profit debility an efficiency it will vastly expand summit comers rash network in the token metropolitan area where it has been relatively weak but financial analysts panelist's a divided on whether an how quickly the gamble will pay off some said soon with our may have played too much for high ever so wine view of the band smoke in view of the smaller bans large debts however ... Others argue the merger was more cost if it is than creating a comparable france that of men scratch ... Analysts agree the ban was a crescent it has expanded overseas enter the lucrative security is business and geared for domestic compression ... But the question the wisdom of some of these ... Moves ... They made bold moves to put everything in place nurse largely year to the hands said decline were ... Benson ltd ... Financial analyst assignments mixer among sumita owns problem us are limits placed on its move twenty u.s. security is business met taking a strain in american nuns one man quote means tax income ... Sumita comers said ... So with our last august agreed to pay five wonder million dollars for occur pun five person partners in the pan ... For the time being it ease the federal reserve board has for them to exchange personal or increase the business they do with each other ... Tie up his idly low dole ... As a lame duck because ... Fate worse stricter than sumita omar expected said one and list comma its shoes said the move will pay off in three years portent is the train differs inch ... Comma a source said ... Kumars is willing to be patient about poser with roots in to the security bill facets topped her tikal sixty five of security is an exchange arc japan's version of the u.s. glass t will act separates commercial from invest when banking put the walls between the two were crumbling income much is said ... He hopes further deregulation will create new of fetching teas we need to find new business chances in recent ... Some cases these will be security is related in some cases trust pan create it that us the kind of deregulation ... Until such changes occur so may come will focus on such to my domestic securities business as profitable governing bond dealing ants in thin ... Relations in my coca security is company relate ... In which each it holds a five passenger who walks a said ... He said sumita jammu is cautiously of a misty were entering the securities business here through its vis a universal ban subsidiary banker del khata all ... Financial ministry is expected to grant licenses to security subsidiaries of u.s. commercial ban soon ... But comma soon is electing to pushed hard for his imminent decision on naam murtaad the subsidiary in on want to make waves week spent this will bailout into three assists taker the city man so much so in sumita roma is also pushing to expand lending to individuals and small in business small in media business history plays disappearing demand from the businesses he at ... The analysts said soon know will have to devote a long tend to ... Digest in its most asian most we sent initiatives including a merger with ailing have some of ... Its sulu dam has been told has been bold in its strategies line works mix ... After that it said question of absorbing and circling around will be the next decade before receive such strategies right along right ... Hoof\")]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/corn/14832', 'A highly trade deficit widened in first court to ... The islands trade deficit why did to four point five billion baht in the first quarter of nineteen eighty seven from to open one billion a year ago the business economic walks departments if ... It said january march imports rose to sixty five fun one billion power ... From fifty eight point seven believe ... Thailand its improved business climate this year resulted in a quaint is seven per cent increase in imports of raw materials fence in may finish power products ... The countries oil import bill however fell twenty three percent in the first quarter due to lower oil prizes ... The department said first cotton exports expanded to sixty point six million baht of fifty six points six billion ... Export growth for smaller than expected due to lower on earnings from many key commodities including rise whose earnings decline eighteen percent maize sixty six percent sugar forty-five percent tin friend the six percent in can pineapple seven percent ... Products with eased in high export growth a jury up sixty four percent clothing fifty seven percent and rubber the eightyfive press')]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/crude/14829', \"Japan planned to revise long-term energy demands on wall ... The ministry of inter national trading industry ... Came iit a ia will revise its long term energy supply demand outlook by august to meet a forecast down training japanese energy demand dennis she officials ever walk mit eyeing woes is expected to lower the projections of primary energy supply is in the year two thousand to five fifty million who kill or letters ... From sick sender million they said ... The decision follows the emerged as the stuff chiller changes in japanese industry following the rise in the rally of the gene ... And a decline in domestic electric power demand ... Midi is planning to work out a revised energy supply demand outlook should deliberations of committee meetings of the agency unnatural if resource is an energy its may official said ... Who they said midi the also review the break down of energy supply sources including oil nuclear coal and natural gas nuclear energy provided the bulk of japan's electric power we the fiscal year ended march thirty one who ... Supplying an estimated friend t seven percent on a kilo but akbar are basis ... Followed by oil the train pithy per cent and liquid fight natural gas off into one percent if they noted\")]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/earn/14859.txt', 'M.k. proposes to for five bonus shearer shook am tale ltd said it proposes to make up to for five bonus issue out awaits revaluation reserves to shareholders resist at mecca and six ... Share holders will be asked to approve the issue and an increase an authorised capital to one seventy five million shares from one twenty five million at a general meeting on may one it said in a statement in new shares al ranks for dividends declared after cooper thirty one ... Camp tale in which bee eight key industries private limited holds a forty one percent stake said it does not expect to maintain its latest annual dividend rate to twenty nine cents a share on the enlarged capita ')]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/grain/14828.txt', \" ... China daily says for many seven took a percent in stocks a survey of nineteen provinces and seven cities showed our men consume between seven and twelfth percent of china's grain stocks the china daily said calls asset that each year one point five seven five million tons or twenty five percent of china's route or put are less put out a to point one billion turtle up to thirty percent which which opens a paper blame the waste on inadequate storage in bad preservation math it said the government had and launched a national programme reduce waste calling or improves technology in store nation preservation and greater court auction off and it gives the paper gave no fur the details\")]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/interest/14849.txt', 'Bonn does by inculcate six point one billion marks in tender ... The bonn dismaying accepted bits or six point one billion mark set to des tender for a twenty eight t securities repurchase pact at of six to eight of three point eight percent a central bank spokesman said banks which bet for duck throttle to else point two billion marks liquidity will be credited with the funds allocated today at must buy back security is pledged on may six some fourteen point nine billion marks will drain from the market today as an earlier pact expires to the bundles bank is effectively with drawing an at it point one billion marks from the market with two days allocation ... A boat does bank spokesmen said an answer to inquiries a to withdrawal of funds to not reflect tightening of credit policy but was to be seen in the context of plentiful liquidity in the banking system ... Banks held an average frisky nine point three billion marks at the bonn does bank over the first six days a command well clearer the likely april magnum reserve requirement of whiskey one billion marks the bonn does bank spokesman noted that by bidding more lethal point two billion marks a below the outgoing fourteen point nine billiard banks then sells at shown they felt they had plenty of liquidity dealers said the bundles bank skin to prevent to bush liquidity at throwing in a market as that would plant the effectiveness so the security repurchase agreement its main open market instrument the steering market interest rates to further pats are likely this month who the next two weeks bondage bank is currently steering call money between three point six in three point it percent all the short-term clutter nation outside that range a possible dealers said a\\r\\n')]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/money/14849', 'Bondage bang are locate six points one billion marks ... In tender vande is accepted baits was six appoint one billion marks at today is tendered for twenty eight days sick kill tease repurchase parts deter fix a rate of three point it per cent a central bang spokesman said a planks which bid for a total ... To twelve point two billion marks liquidity will be credited with the funds allocated today and must buy back security is placed on may six some fourteen per nine billion marks will drain from the market today has an earlier pact expires to the bondage plank is effective with trying innate ate one fund billion marks him the market with to days allocation abundance bank spokesman said in answer to inquiries had the withdrawal of fun did nasty flick at tightening of credit policy but was seen ... In the context of plentiful liquidity of in the banking system fangs held an average fifty nine per three billion marks at the band is man over the first six days of the month will clear the likely april minimum reserve for fifty one billion marks no one is ban spokesman noted that by bidding on literal when to began mass below the out going fourteen won nine billion man next themselves have shown the film they had plenty of liquidity dealers said abundance bank is keen to prevent too much liquidity accruing in the market as that could blunt the effectiveness the of the security repurchase acclaim in its pane open market instrument first winning market interest rates to for the pact the likely this month over the next week mondays thank his country steering call money beating three points six in three point eight percent although the short of fluctuations outside the sinn the pause in the lessened')]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/ship/14839.txt', ' ... Australian foreign shiv ban and spot in an nest of up roads hit up cruz in new south pals with to dr and western australia yesterday lifted their ban on foreign flag ships carrying containers part new south while sports are still being this erupt at by a separate dispute shipping sources said ban imposed toe week ago over a p claim had prevented the movement in or out of toe of nearly twenty vessels they say the payed dispute to went before a hearing of the arbitration commission today meanwhile disruption began to date to cargo handling in the ports of sydney newcastle and thought camilla this said the industrial action at the end as the blue ports is part of the week of action called by the end as the blue threads and labor counsel to protest changes to the states for cuts compensation laws the shipping sources said the various port unions appear to be taking it in turn to work for a short time if at the start of each shift and then to walk of cargo handling in the ports has been dress up dead with container movements most affected but has not stopped altogether they said they said they could tot say how long the disruption will go on and what effect it will have on shipping movements')]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/trade/14826.txt', \"Edition reporters fear damage from the u.s. japan rao refer to mounting threat friction between the u.s. and japan has raised fears among many of ensures exporting nations that the role could inflict far-reaching economic damage businessman an official said they told raw utter correspondence in asian capitals are you were small of address japan might ghost protectionist sentiment in the u.s. and jeeps to call boots on american imports of the product but some exporters said that while the cuts late would halt them in the longer run in the short of to cures loss might be on their gain the u.s. has weighed it will impose three hundred million dollars of tariffs on imports of japanese electronics quotes on april seventeen in retaliation for japan's allies feel it to stick to of pact not to sell semi conductors on vault markets and too low cost on official japanese estimates put dow impact of paris at c ten billion dollars and specks month for major electronics from said they or for ritually halt exports of products hit by the new texas we would not be able to do business aid a spokesman for leading japanese electronics for march to ship the electrical in terrestrial company ltd if the tariffs remain in place for any length of time beyond a few merge it feel meet the complete you notion of exports of goods objects to tariffs if to the u.s. said tom the tha is top cannon list at the two key office of top broker and gems capital and comb in taiwan businessman an official sar also four-day to eye wear a pair of the cds nests of de u.s. threat at as japan because it sauce as a warning to our fate a senior taiwanese trade officials who asked not to be named tie one had a trade ... Surplus of fifteen point six billion dollars last year ninety five percent of it which the u.s. the surplus help smiled i was foreign exchange is up to fifty three billion dollars among the votes large to fee must quickly open our markets remove skull grade barriers cut import tariffs to allow imports of you bus products if we want to diffused problems from possible u.s. retaliation said paul shame if chairman of textile exports and taiwan saves group a senior official of south korea is trade promotion association safe the tread dispute bit when the lever's and chapatis white also leaked to pressure on south korea was chief exports are similar to those of japan last year south korea had threat surplus of seven point ... Mess of from four point nine billion dollars a nineteen eighty five in malaysia i read officers and businessmen said ... Tough cops at and japan might allow hard hit producers of semi conduct as in third countries to expand their sales to the u.s. in honks where newspaper some alleged japan has been say dim below cost amid courter some electronics manufacture shared that few but other businessmen say that such a short tub comma shiv advantage would be out wide buy for that u.s. pressure to block imports that is a very short a view said lawless mills directed gen of adoration of farm corn industry if the whole purpose is to prevent imports one day it will pay extended to all other sources much more serious for harm cord is a disadvantage of action his treading trade his it the youth was last year was honed coaxed thickest exports market according for ever third devotes a the domestically produced to exports the australian government is awaiting the outcome of crack talks bitter in the u.s. and japan with interest and concern in the estimate rested john porter said it can barrel last friday this kind of deterioration in trudge relation patent to cut rich richer major fitting part nurse of ores is a various serious mutter button said he said australias concerns centered on co bull and beef ostrich two largest x or such a path also a significant u.s. exports to that country been filed even as japanese diplomat if matter was to solve the traits stand of khatik japan's ruling liberal democratic party yesterday outlined a package of economic measures to bust japanese economy the measures proposed include its large supplementary budget and away call public walks pending in the first half of the financial year they also call for step up spending as an emergency measure to stimulate economic despite prime minister dr so hero nath costs on afford fiscal reform throw crow deputy u.s. study presented if michael smith an meccan to crow the japan's deputy minister of international tread an industry heart due to meet in wash enter this week in an effort to and the dispute\")]\n",
      "[('file:/home/enterprise.internal.city.ac.uk/acvn747/transcriptions/wheat/14841.txt', ' ... Srilankan gets you s.d. approval for wheat thrice fire department officials said the u.s. department of agriculture approve p continental crane co sell off fifty two thousand five funded tons office of to be at each team nine new west dollars a tan c and as from pacific northwest to colombo they said the shipment was for april eke to twenty deliver')]\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 21\n",
      "Number of documents read is: 20\n",
      "Number of documents read is: 201\n"
     ]
    }
   ],
   "source": [
    "## Initialise Spark and create paths for full file as well as chosen topics for classification.\n",
    "# Importing necessary pyspark modules and attributes.\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql import *\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Generate the SparkSession builder.\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Creating an absolute path for the full set.\n",
    "# Absolute path required, as the executors that will read them from directories, will not run in the same environment, \n",
    "# so that %cd calls don't help.\n",
    "import os.path\n",
    "p = os.path.abspath('./transcriptions')\n",
    "print(\"p: \", p) \n",
    "\n",
    "# Set the path to select all 10 found topics.\n",
    "dirPath1 = p + '/acq/'\n",
    "dirPath2 = p + '/corn/'\n",
    "dirPath3 = p + '/crude/'\n",
    "dirPath4 = p + '/earn/'\n",
    "dirPath5 = p + '/grain/'\n",
    "dirPath6 = p + '/interest/'\n",
    "dirPath7 = p + '/money/'\n",
    "dirPath8 = p + '/ship/'\n",
    "dirPath9 = p + '/trade/'\n",
    "dirPath10 = p + '/wheat/'\n",
    "\n",
    "print(dirPath1)\n",
    "print(dirPath2)\n",
    "print(dirPath3)\n",
    "print(dirPath4)\n",
    "print(dirPath5)\n",
    "print(dirPath6)\n",
    "print(dirPath7)\n",
    "print(dirPath8)\n",
    "print(dirPath9)\n",
    "print(dirPath10)\n",
    "\n",
    "# Use wholeTextFiles to read both the files.\n",
    "acq_rdd = sc.wholeTextFiles(dirPath1)\n",
    "corn_rdd = sc.wholeTextFiles(dirPath2)\n",
    "crude_rdd = sc.wholeTextFiles(dirPath3)\n",
    "earn_rdd = sc.wholeTextFiles(dirPath4)\n",
    "grain_rdd = sc.wholeTextFiles(dirPath5)\n",
    "interest_rdd = sc.wholeTextFiles(dirPath6)\n",
    "money_rdd = sc.wholeTextFiles(dirPath7)\n",
    "ship_rdd = sc.wholeTextFiles(dirPath8)\n",
    "trade_rdd = sc.wholeTextFiles(dirPath9)\n",
    "wheat_rdd = sc.wholeTextFiles(dirPath10)\n",
    "\n",
    "print(acq_rdd.take(1))\n",
    "print(corn_rdd.take(1))\n",
    "print(crude_rdd.take(1))\n",
    "print(earn_rdd.take(1))\n",
    "print(grain_rdd.take(1))\n",
    "print(interest_rdd.take(1))\n",
    "print(money_rdd.take(1))\n",
    "print(ship_rdd.take(1))\n",
    "print(trade_rdd.take(1))\n",
    "print(wheat_rdd.take(1))\n",
    "\n",
    "# Create a union of all rdd's so we hava a full set.\n",
    "combined_RDD = acq_rdd.union(corn_rdd).union(crude_rdd).union(earn_rdd).union(grain_rdd).union(interest_rdd).union(money_rdd).union(ship_rdd).union(trade_rdd).union(wheat_rdd)\n",
    "\n",
    "# Print the total number of documents.\n",
    "print ('Number of documents read is:',acq_rdd.count())\n",
    "print ('Number of documents read is:',corn_rdd.count())\n",
    "print ('Number of documents read is:',crude_rdd.count())\n",
    "print ('Number of documents read is:',earn_rdd.count())\n",
    "print ('Number of documents read is:',grain_rdd.count())\n",
    "print ('Number of documents read is:',interest_rdd.count())\n",
    "print ('Number of documents read is:',money_rdd.count())\n",
    "print ('Number of documents read is:',ship_rdd.count())\n",
    "print ('Number of documents read is:',trade_rdd.count())\n",
    "print ('Number of documents read is:',wheat_rdd.count())\n",
    "\n",
    "print ('Number of documents read is:',combined_RDD.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acq', \"\\r\\n... Soon with the mood and insert quick recovery from much soon is certain lose its status as chip warns most profitable bank cuts result of its merger with the high were so woman financial analysis said osaka based sony to move with ... Deposits of a run print three pun nine trillion yen emerged with i were so ago ... A small struggling pant with tennis timid in one pen to nine billion dollars enough in un recovery billions ... Knocked over ... When despite the link talk soon with some of president co kumar sue told reuters is confident his banking quickly regain its position ... Will be back ... In position in first place within three years comma so said none intervened he said that while the merger will find will initially reduce omit the most profit debility an efficiency it will vastly expand summit comers rash network in the token metropolitan area where it has been relatively weak but financial analysts panelist's a divided on whether an how quickly the gamble will pay off some said soon with our may have played too much for high ever so wine view of the band smoke in view of the smaller bans large debts however ... Others argue the merger was more cost if it is than creating a comparable france that of men scratch ... Analysts agree the ban was a crescent it has expanded overseas enter the lucrative security is business and geared for domestic compression ... But the question the wisdom of some of these ... Moves ... They made bold moves to put everything in place nurse largely year to the hands said decline were ... Benson ltd ... Financial analyst assignments mixer among sumita owns problem us are limits placed on its move twenty u.s. security is business met taking a strain in american nuns one man quote means tax income ... Sumita comers said ... So with our last august agreed to pay five wonder million dollars for occur pun five person partners in the pan ... For the time being it ease the federal reserve board has for them to exchange personal or increase the business they do with each other ... Tie up his idly low dole ... As a lame duck because ... Fate worse stricter than sumita omar expected said one and list comma its shoes said the move will pay off in three years portent is the train differs inch ... Comma a source said ... Kumars is willing to be patient about poser with roots in to the security bill facets topped her tikal sixty five of security is an exchange arc japan's version of the u.s. glass t will act separates commercial from invest when banking put the walls between the two were crumbling income much is said ... He hopes further deregulation will create new of fetching teas we need to find new business chances in recent ... Some cases these will be security is related in some cases trust pan create it that us the kind of deregulation ... Until such changes occur so may come will focus on such to my domestic securities business as profitable governing bond dealing ants in thin ... Relations in my coca security is company relate ... In which each it holds a five passenger who walks a said ... He said sumita jammu is cautiously of a misty were entering the securities business here through its vis a universal ban subsidiary banker del khata all ... Financial ministry is expected to grant licenses to security subsidiaries of u.s. commercial ban soon ... But comma soon is electing to pushed hard for his imminent decision on naam murtaad the subsidiary in on want to make waves week spent this will bailout into three assists taker the city man so much so in sumita roma is also pushing to expand lending to individuals and small in business small in media business history plays disappearing demand from the businesses he at ... The analysts said soon know will have to devote a long tend to ... Digest in its most asian most we sent initiatives including a merger with ailing have some of ... Its sulu dam has been told has been bold in its strategies line works mix ... After that it said question of absorbing and circling around will be the next decade before receive such strategies right along right ... Hoof\"), ('acq', \" ... Bon rightd corp still conceal dinner at less mining bail out ... Bond core holdings ltd and at less consolidated mining and development call ... A still holding talks annum bailout package for the troubled mining for ... In artless statements it at less the philippines biggest copper producer said it had been hit by depressed world copper prizes to reported a net loss of nine sensex point three eight million pay sows the year ending ... One nine eight six compared with a net loss a one point five three billion in nine won nine eight fire ... Company said it had been able to cut its losses because it scale down ... Copper operations literal island of save ... Started din the second half in eighty six ... At listed never stations were contained one the acquisition by bond of companies existing mine loans and they re stretching enter gold ... A memorandum of understanding sign by the two sites in of gobble austrian said bond would acquire lack losses total loans of to seven five million dollars to be repaid by the mining company in gold ... At less said the two sides were also discussing it quick infusion and dart listen creation of a development fund for further exploration and development of the companies coal properties in the center province a match pay ... Vision bankers general manager of bond cause international in haunt count told writer's chief at le statement on the go seshan was teenage reasonably r.k. live band said bond cough was seriously considering several investment in the philippines but did not give it ... In its statement at listen development of the free world war ii on the ground mines in mass bait had been accelerated in the auto nature increased extending the operations life at least until nineteen ninety three\")]\n"
     ]
    }
   ],
   "source": [
    "## Split the filename and content using splitFileWords.\n",
    "# This function helps split the filename and content.\n",
    "\n",
    "import re \n",
    "    \n",
    "def splitFileWords(filenameContent): #  splitting function\n",
    "    f,c = filenameContent # split the input tuple  \n",
    "    fwLst = [] # the new list for (filename,word) tuples\n",
    "    wLst = re.split('\\W+',c) # create a word list wLst\n",
    "    for w in wLst : # iterate through the list\n",
    "        fwLst.append((f,w)) # append (f,w)\n",
    "    return fwLst # return a concise list of (f,w) tuples\n",
    "\n",
    "\n",
    "# Remove the file name and path before the last directory name (i.e. the 'combined' name).\n",
    "fnt_RDD = combined_RDD.map(lambda ft: (re.split('[/]',ft[0])[-2],ft[1]))\n",
    "print(fnt_RDD.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acq',\n",
       "  \"\\r\\n... Soon with the mood and insert quick recovery from much soon is certain lose its status as chip warns most profitable bank cuts result of its merger with the high were so woman financial analysis said osaka based sony to move with ... Deposits of a run print three pun nine trillion yen emerged with i were so ago ... A small struggling pant with tennis timid in one pen to nine billion dollars enough in un recovery billions ... Knocked over ... When despite the link talk soon with some of president co kumar sue told reuters is confident his banking quickly regain its position ... Will be back ... In position in first place within three years comma so said none intervened he said that while the merger will find will initially reduce omit the most profit debility an efficiency it will vastly expand summit comers rash network in the token metropolitan area where it has been relatively weak but financial analysts panelist's a divided on whether an how quickly the gamble will pay off some said soon with our may have played too much for high ever so wine view of the band smoke in view of the smaller bans large debts however ... Others argue the merger was more cost if it is than creating a comparable france that of men scratch ... Analysts agree the ban was a crescent it has expanded overseas enter the lucrative security is business and geared for domestic compression ... But the question the wisdom of some of these ... Moves ... They made bold moves to put everything in place nurse largely year to the hands said decline were ... Benson ltd ... Financial analyst assignments mixer among sumita owns problem us are limits placed on its move twenty u.s. security is business met taking a strain in american nuns one man quote means tax income ... Sumita comers said ... So with our last august agreed to pay five wonder million dollars for occur pun five person partners in the pan ... For the time being it ease the federal reserve board has for them to exchange personal or increase the business they do with each other ... Tie up his idly low dole ... As a lame duck because ... Fate worse stricter than sumita omar expected said one and list comma its shoes said the move will pay off in three years portent is the train differs inch ... Comma a source said ... Kumars is willing to be patient about poser with roots in to the security bill facets topped her tikal sixty five of security is an exchange arc japan's version of the u.s. glass t will act separates commercial from invest when banking put the walls between the two were crumbling income much is said ... He hopes further deregulation will create new of fetching teas we need to find new business chances in recent ... Some cases these will be security is related in some cases trust pan create it that us the kind of deregulation ... Until such changes occur so may come will focus on such to my domestic securities business as profitable governing bond dealing ants in thin ... Relations in my coca security is company relate ... In which each it holds a five passenger who walks a said ... He said sumita jammu is cautiously of a misty were entering the securities business here through its vis a universal ban subsidiary banker del khata all ... Financial ministry is expected to grant licenses to security subsidiaries of u.s. commercial ban soon ... But comma soon is electing to pushed hard for his imminent decision on naam murtaad the subsidiary in on want to make waves week spent this will bailout into three assists taker the city man so much so in sumita roma is also pushing to expand lending to individuals and small in business small in media business history plays disappearing demand from the businesses he at ... The analysts said soon know will have to devote a long tend to ... Digest in its most asian most we sent initiatives including a merger with ailing have some of ... Its sulu dam has been told has been bold in its strategies line works mix ... After that it said question of absorbing and circling around will be the next decade before receive such strategies right along right ... Hoof\"),\n",
       " ('acq',\n",
       "  \" ... Bon rightd corp still conceal dinner at less mining bail out ... Bond core holdings ltd and at less consolidated mining and development call ... A still holding talks annum bailout package for the troubled mining for ... In artless statements it at less the philippines biggest copper producer said it had been hit by depressed world copper prizes to reported a net loss of nine sensex point three eight million pay sows the year ending ... One nine eight six compared with a net loss a one point five three billion in nine won nine eight fire ... Company said it had been able to cut its losses because it scale down ... Copper operations literal island of save ... Started din the second half in eighty six ... At listed never stations were contained one the acquisition by bond of companies existing mine loans and they re stretching enter gold ... A memorandum of understanding sign by the two sites in of gobble austrian said bond would acquire lack losses total loans of to seven five million dollars to be repaid by the mining company in gold ... At less said the two sides were also discussing it quick infusion and dart listen creation of a development fund for further exploration and development of the companies coal properties in the center province a match pay ... Vision bankers general manager of bond cause international in haunt count told writer's chief at le statement on the go seshan was teenage reasonably r.k. live band said bond cough was seriously considering several investment in the philippines but did not give it ... In its statement at listen development of the free world war ii on the ground mines in mass bait had been accelerated in the auto nature increased extending the operations life at least until nineteen ninety three\"),\n",
       " ('acq',\n",
       "  ' ... See a resold for his gold for seventy six million dollars when ... Greek we increase consolidated said the consortium it is leading will pay incentive six one five a million dollars for the acquisition of c r.a. limited its ... Forest gold ... Bt while ltd ... Unit reported district see r.a. and rim greek did not disclose the price state fee increase will hold seven ... Forty four percent of the consortium wise ... Was swim resource ill will hold twenty seven percent ... And crows cs mining plenty when cassette it said in a statement as reported forest cold owns two mines in western australia produced in a combined ... Set is seven thousand allowances of golden year litter also owns none developed gold project a write up wharf half')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing steps required to clean the data appropriately- the removal of headers.\n",
    "\n",
    "import re\n",
    "\n",
    "# Define new function to remove the headers using regular expressions.\n",
    "def removeHeader(ft): \n",
    "    fn,text = ft # Unpack the filename and text content.\n",
    "    \n",
    "    # Use a regular expression to match the text.\n",
    "    matchObj = re.match(r'.+^Lines:(.*)', text,re.DOTALL|re.MULTILINE) \n",
    "    if(matchObj): # Only if the pattern has matched...\n",
    "        text = matchObj.group(1) # ...can we replace the text, otherwise the old pattern is returned.\n",
    "    return (fn,text)\n",
    "\n",
    "fnt_RDD2 = fnt_RDD.map(removeHeader)\n",
    "fnt_RDD2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.9</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Importing required dataframe modules.\n",
    "import pixiedust\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(topic,StringType,true), StructField(text,StringType,true)]\n"
     ]
    }
   ],
   "source": [
    "# The schema is encoded in a string- we are only interested in the topic and text.\n",
    "schemaString = \"topic text\"\n",
    "\n",
    "# A StructField object comprises of three fields, name (a string), dataType (a DataType) and nullable (a bool).\n",
    "# Create 2 fields of strings with names according to our schemaString.\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acq', \"\\r\\n... Soon with the mood and insert quick recovery from much soon is certain lose its status as chip warns most profitable bank cuts result of its merger with the high were so woman financial analysis said osaka based sony to move with ... Deposits of a run print three pun nine trillion yen emerged with i were so ago ... A small struggling pant with tennis timid in one pen to nine billion dollars enough in un recovery billions ... Knocked over ... When despite the link talk soon with some of president co kumar sue told reuters is confident his banking quickly regain its position ... Will be back ... In position in first place within three years comma so said none intervened he said that while the merger will find will initially reduce omit the most profit debility an efficiency it will vastly expand summit comers rash network in the token metropolitan area where it has been relatively weak but financial analysts panelist's a divided on whether an how quickly the gamble will pay off some said soon with our may have played too much for high ever so wine view of the band smoke in view of the smaller bans large debts however ... Others argue the merger was more cost if it is than creating a comparable france that of men scratch ... Analysts agree the ban was a crescent it has expanded overseas enter the lucrative security is business and geared for domestic compression ... But the question the wisdom of some of these ... Moves ... They made bold moves to put everything in place nurse largely year to the hands said decline were ... Benson ltd ... Financial analyst assignments mixer among sumita owns problem us are limits placed on its move twenty u.s. security is business met taking a strain in american nuns one man quote means tax income ... Sumita comers said ... So with our last august agreed to pay five wonder million dollars for occur pun five person partners in the pan ... For the time being it ease the federal reserve board has for them to exchange personal or increase the business they do with each other ... Tie up his idly low dole ... As a lame duck because ... Fate worse stricter than sumita omar expected said one and list comma its shoes said the move will pay off in three years portent is the train differs inch ... Comma a source said ... Kumars is willing to be patient about poser with roots in to the security bill facets topped her tikal sixty five of security is an exchange arc japan's version of the u.s. glass t will act separates commercial from invest when banking put the walls between the two were crumbling income much is said ... He hopes further deregulation will create new of fetching teas we need to find new business chances in recent ... Some cases these will be security is related in some cases trust pan create it that us the kind of deregulation ... Until such changes occur so may come will focus on such to my domestic securities business as profitable governing bond dealing ants in thin ... Relations in my coca security is company relate ... In which each it holds a five passenger who walks a said ... He said sumita jammu is cautiously of a misty were entering the securities business here through its vis a universal ban subsidiary banker del khata all ... Financial ministry is expected to grant licenses to security subsidiaries of u.s. commercial ban soon ... But comma soon is electing to pushed hard for his imminent decision on naam murtaad the subsidiary in on want to make waves week spent this will bailout into three assists taker the city man so much so in sumita roma is also pushing to expand lending to individuals and small in business small in media business history plays disappearing demand from the businesses he at ... The analysts said soon know will have to devote a long tend to ... Digest in its most asian most we sent initiatives including a merger with ailing have some of ... Its sulu dam has been told has been bold in its strategies line works mix ... After that it said question of absorbing and circling around will be the next decade before receive such strategies right along right ... Hoof\")]\n"
     ]
    }
   ],
   "source": [
    "# These parameters combined create the defined schema.\n",
    "schema = StructType(fields)\n",
    "\n",
    "\n",
    "print(fnt_RDD.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sometimes, when you have multiple notebooks open at the same time, you might get an error that the metastore_db is not accessible.\n",
    "# We can not prevent this form happening on DSX (apart from not opening more than one notebook at a time).\n",
    "# If it does happen you need to delete the metastore_db. The path of the metastore_db is in the error messages and it's typically \n",
    "# lond like this example: \n",
    "# /gpfs/global_fs01/sym_shared/YPProdSpark/user/s832-dfe96c6e1f1d61-70d619a53771/notebook/jupyter-rt/kernel-cdcf5f73-9afb-481d-ac40-a210a649eb69-20180222_154448/metastore_db\n",
    "# once you have it, you can use it with !rm -Rf to delete it:\n",
    "#!rm -Rf <Put the path of the metastore_db here>\n",
    "!rm -Rf /home/enterprise.internal.city.ac.uk/acvn747/metastore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the schema in createDataFrame, to create a DataFrame 'df' from the updated and header-removed RDD.\n",
    "df = sqlContext.createDataFrame(fnt_RDD, schema)\n",
    "\n",
    "# Print the schema of our DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "barChart",
      "rendererId": "matplotlib"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>\n",
       "        <div class=\"pd_save is-viewer-good\" style=\"padding-right:10px;text-align: center;line-height:initial !important;font-size: xx-large;font-weight: 500;color: coral;\">\n",
       "            \n",
       "        </div>\n",
       "    <div id=\"chartFigured70d1822\" class=\"pd_save is-viewer-good\" style=\"overflow-x:auto\">\n",
       "            \n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use pixiedust to show the number of topics by frequency.\n",
    "display(df.select('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   topic|\n",
      "+--------+\n",
      "|     acq|\n",
      "|    ship|\n",
      "|   money|\n",
      "|    earn|\n",
      "|interest|\n",
      "|    corn|\n",
      "|   wheat|\n",
      "|   trade|\n",
      "|   grain|\n",
      "|   crude|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view using the DataFrame, enabling use of SparkSQL.\n",
    "df.createOrReplaceTempView(\"transcripts\")\n",
    "\n",
    "# SQL can now be run on the DataFrame.\n",
    "# Start by selecting only the topics elements of each row.\n",
    "results = sqlContext.sql(\"SELECT DISTINCT topic FROM transcripts\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|   topic|cnt|\n",
      "+--------+---+\n",
      "|   trade| 21|\n",
      "|     acq| 20|\n",
      "|   money| 20|\n",
      "|    ship| 20|\n",
      "|    earn| 20|\n",
      "|interest| 20|\n",
      "|    corn| 20|\n",
      "|   wheat| 20|\n",
      "|   grain| 20|\n",
      "|   crude| 20|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Utilising SQL and using topic names as a distinct feature, show a count  of the number of files/documents.\n",
    "results_topic = sqlContext.sql(\"SELECT DISTINCT topic, count(*) as cnt FROM transcripts GROUP BY topic ORDER BY cnt DESC\")\n",
    "results_topic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Label creation.\n",
    "# df.withColumn returns a new DataFrame by adding a column with a name and value for each row.\n",
    "# The value is a 'column expression', where we compare with the string 'acq' for example, \n",
    "# to find out whether the topic is about acq (acquisitions).\n",
    "# \"double\" converts the resulting Boolean value into a number.\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"acq%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"corn%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"crude%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"earn%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"grain%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"interest%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"money%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"ship%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"trade%\").cast(\"double\"))\n",
    "news_Topics = df.withColumn(\"label\",df.topic.like(\"wheat%\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|  acq|\n",
      "... Soon with t...|  0.0|\n",
      "|  acq| ... Bon rightd c...|  0.0|\n",
      "|  acq| ... See a resold...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "| corn|A highly trade de...|  0.0|\n",
      "| corn| ... Japan minist...|  0.0|\n",
      "| corn| ... Zambian does...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|crude|Japan planned to ...|  0.0|\n",
      "|crude|Energy u.s. petro...|  0.0|\n",
      "|crude|Turkey calls for ...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "| earn|M.k. proposes to ...|  0.0|\n",
      "| earn| ... Back ... Doe...|  0.0|\n",
      "| earn|... Go watt indus...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|grain| ... China daily ...|  0.0|\n",
      "|grain|... It high trade...|  0.0|\n",
      "|grain|... Srilanka gets...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+--------------------+-----+\n",
      "|   topic|                text|label|\n",
      "+--------+--------------------+-----+\n",
      "|interest|Bonn does by incu...|  0.0|\n",
      "|interest|... Use  are ... ...|  0.0|\n",
      "|interest|... Econ what aus...|  0.0|\n",
      "+--------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|money|Bondage bang are ...|  0.0|\n",
      "|money|... U k money mar...|  0.0|\n",
      "|money|... Economics bou...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "| ship| ... Australian f...|  0.0|\n",
      "| ship| ... Australian f...|  0.0|\n",
      "| ship|Independent chair...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|trade|Edition reporters...|  0.0|\n",
      "|trade| ... A high credi...|  0.0|\n",
      "|trade| ... Say tan mini...|  0.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+--------------------+-----+\n",
      "|topic|                text|label|\n",
      "+-----+--------------------+-----+\n",
      "|wheat| ... Srilankan ge...|  1.0|\n",
      "|wheat|For their argenti...|  1.0|\n",
      "|wheat| ... On to those ...|  1.0|\n",
      "+-----+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Use a syntax similar to an array to select some examples of either class.\n",
    "topic_acq_df = news_Topics[df.topic.like(\"acq%\")]\n",
    "topic_acq_df.show(3)\n",
    "topic_corn_df = news_Topics[df.topic.like(\"corn%\")]\n",
    "topic_corn_df.show(3)\n",
    "topic_crude_df = news_Topics[df.topic.like(\"crude%\")]\n",
    "topic_crude_df.show(3)\n",
    "topic_earn_df = news_Topics[df.topic.like(\"earn%\")]\n",
    "topic_earn_df.show(3)\n",
    "topic_grain_df = news_Topics[df.topic.like(\"grain%\")]\n",
    "topic_grain_df.show(3)\n",
    "topic_interest_df = news_Topics[df.topic.like(\"interest%\")]\n",
    "topic_interest_df.show(3)\n",
    "topic_money_df = news_Topics[df.topic.like(\"money%\")]\n",
    "topic_money_df.show(3)\n",
    "topic_ship_df = news_Topics[df.topic.like(\"ship%\")]\n",
    "topic_ship_df.show(3)\n",
    "topic_trade_df = news_Topics[df.topic.like(\"trade%\")]\n",
    "topic_trade_df.show(3)\n",
    "topic_wheat_df = news_Topics[df.topic.like(\"wheat%\")]\n",
    "topic_wheat_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document count: 201\n",
      "Training-set count: 141\n",
      "Test-set count: 60\n"
     ]
    }
   ],
   "source": [
    "## Create the training and testing sets from the dataframe above.\n",
    "# randomSplit - splits the Df into training/testing using the weights 0.7 for training and 0.3 for testing.\n",
    "train_set, test_set = news_Topics.randomSplit([0.7, 0.3], 123)\n",
    "print (\"Total document count:\",news_Topics.count())\n",
    "print (\"Training-set count:\",train_set.count())\n",
    "print (\"Test-set count:\",test_set.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## ML Pipeline creation\n",
    "# Import required modules for algorithms and evaluators, in addition to ParamGridBuilder.\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n",
    "\n",
    "# Constructing the pipeline.\n",
    "# Split each sentence into words using Tokenizer via white spaces only.\n",
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "# Remove stopwords.\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "\n",
    "# For each sentence (bag of words),use HashingTF to hash the sentence into a feature vector. \n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "\n",
    "# Use IDF to rescale the feature vectors; this generally improves performance when using text as features.\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "\n",
    "# Feature vectors then  passed to a learning algorithm.\n",
    "lr = LogisticRegression()\n",
    "nb = NaiveBayes()\n",
    "rf = RandomForestClassifier()\n",
    "    \n",
    "# Connect these steps to create the final pipeline.\n",
    "pipeline=Pipeline(stages=[tokenizer,remover,hashingTF,idf, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: inputCol: input column name. (current: text)\n",
      "outputCol: output column name. (default: Tokenizer_494ea7dd785f0aa442e8__output, current: words)\n",
      "\n",
      "\n",
      "\n",
      "Remover: caseSensitive: whether to do a case sensitive comparison over the stop words (default: False, current: False)\n",
      "inputCol: input column name. (current: words)\n",
      "outputCol: output column name. (default: StopWordsRemover_47c28b129599996d6dd8__output, current: filtered)\n",
      "stopWords: The words to be filtered out (default: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn'])\n",
      "\n",
      "\n",
      "\n",
      "HashingTF: binary: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False. (default: False)\n",
      "inputCol: input column name. (current: filtered)\n",
      "numFeatures: number of features. (default: 262144, current: 1000)\n",
      "outputCol: output column name. (default: HashingTF_45f18771912eba996691__output, current: rawFeatures)\n",
      "\n",
      "\n",
      "\n",
      "IDF: inputCol: input column name. (current: rawFeatures)\n",
      "minDocFreq: minimum number of documents in which a term should appear for filtering (default: 0, current: 0)\n",
      "outputCol: output column name. (default: IDF_4ab8b5ac3217498b28fa__output, current: features)\n",
      "\n",
      "\n",
      "\n",
      "Pipeline: stages: pipeline stages (current: [Tokenizer_494ea7dd785f0aa442e8, StopWordsRemover_47c28b129599996d6dd8, HashingTF_45f18771912eba996691, IDF_4ab8b5ac3217498b28fa, RandomForestClassifier_42259f2f48d8ba96c07c])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Obtain and display information for each parameter using the .explainParams().\n",
    "print (\"Tokenizer:\",tokenizer.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"Remover:\",remover.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "print (\"HashingTF:\",hashingTF.explainParams())\n",
    "print (\"\\n\\n\")\n",
    "print (\"IDF:\",idf.explainParams())\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print (\"Pipeline:\",pipeline.explainParams())\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "## Use the pipeline option to fit the training set and create a model. \n",
    "\n",
    "# Jupyter offers a simpler way to take the time than we used in the coursework. \n",
    "%time\n",
    "\n",
    "# Following the ML pipeline contruction, fit it to the training data\n",
    "# and obtain a trained pipeline model that can be used for prediction.\n",
    "model=pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|  acq|[0.92618148275936...|       0.0|  0.0|\n",
      "|  acq|[0.94228910022495...|       0.0|  0.0|\n",
      "|  acq|[0.97339364293755...|       0.0|  0.0|\n",
      "|  acq|[0.92027510231809...|       0.0|  0.0|\n",
      "|  acq|[0.97795224596298...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|  acq|[0.92618148275936...|       0.0|  0.0|\n",
      "|  acq|[0.94228910022495...|       0.0|  0.0|\n",
      "|  acq|[0.97339364293755...|       0.0|  0.0|\n",
      "|  acq|[0.92027510231809...|       0.0|  0.0|\n",
      "|  acq|[0.97795224596298...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "| corn|[0.66923602071251...|       0.0|  0.0|\n",
      "| corn|[0.69152771909350...|       0.0|  0.0|\n",
      "| corn|[0.82782538633402...|       0.0|  0.0|\n",
      "| corn|[0.86465046304294...|       0.0|  0.0|\n",
      "| corn|[0.61454679985394...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|crude|[0.96116363463127...|       0.0|  0.0|\n",
      "|crude|[0.82377988302693...|       0.0|  0.0|\n",
      "|crude|[0.90796407893200...|       0.0|  0.0|\n",
      "|crude|[0.91919176898476...|       0.0|  0.0|\n",
      "|crude|[0.96497334087522...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "| earn|[0.85928123278411...|       0.0|  0.0|\n",
      "| earn|[0.83143772106136...|       0.0|  0.0|\n",
      "| earn|[0.96819176898476...|       0.0|  0.0|\n",
      "| earn|[0.96997955968243...|       0.0|  0.0|\n",
      "| earn|[0.97032131371026...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|grain|[0.79360542480647...|       0.0|  0.0|\n",
      "|grain|[0.76629253562617...|       0.0|  0.0|\n",
      "|grain|[0.97262877748831...|       0.0|  0.0|\n",
      "|grain|[0.67387411470592...|       0.0|  0.0|\n",
      "|grain|[0.86292166377669...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------------------+----------+-----+\n",
      "|   topic|         probability|prediction|label|\n",
      "+--------+--------------------+----------+-----+\n",
      "|interest|[0.92519899260580...|       0.0|  0.0|\n",
      "|interest|[0.97403872690489...|       0.0|  0.0|\n",
      "|interest|[0.94442083782271...|       0.0|  0.0|\n",
      "|interest|[0.94358237370168...|       0.0|  0.0|\n",
      "|interest|[0.70383220767215...|       0.0|  0.0|\n",
      "+--------+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|money|[0.97642611429228...|       0.0|  0.0|\n",
      "|money|[0.98124744157532...|       0.0|  0.0|\n",
      "|money|[0.81411534491381...|       0.0|  0.0|\n",
      "|money|[0.83194749855799...|       0.0|  0.0|\n",
      "|money|[0.96322486990223...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "| ship|[0.74274281655458...|       0.0|  0.0|\n",
      "| ship|[0.96819176898476...|       0.0|  0.0|\n",
      "| ship|[0.97432915637215...|       0.0|  0.0|\n",
      "| ship|[0.67472421129392...|       0.0|  0.0|\n",
      "| ship|[0.96939794617399...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|trade|[0.90163562216167...|       0.0|  0.0|\n",
      "|trade|[0.96819176898476...|       0.0|  0.0|\n",
      "|trade|[0.86607196614204...|       0.0|  0.0|\n",
      "|trade|[0.87460025644599...|       0.0|  0.0|\n",
      "|trade|[0.91144707454091...|       0.0|  0.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+----------+-----+\n",
      "|topic|         probability|prediction|label|\n",
      "+-----+--------------------+----------+-----+\n",
      "|wheat|[0.74510515158164...|       0.0|  1.0|\n",
      "|wheat|[0.80801216530013...|       0.0|  1.0|\n",
      "|wheat|[0.65271908020968...|       0.0|  1.0|\n",
      "|wheat|[0.86916022323810...|       0.0|  1.0|\n",
      "+-----+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## After obtaining a fitted pipeline model, assess performance. \n",
    "# Manual checks to display the predicted labels.\n",
    "\n",
    "# Use the .transform() on the test set to make predictions on the test set.\n",
    "test_predictions = model.transform(test_set)\n",
    "train_predictions = model.transform(train_set)\n",
    "\n",
    "# Show the predicted labels along with true labels and raw texts.\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").show(5)\n",
    "# and show some of the other classes. \n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"acq%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"corn%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"crude%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"earn%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"grain%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"interest%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"money%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"ship%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"trade%\")).show(5)\n",
    "test_predictions.select(\"topic\",\"probability\",\"prediction\",\"label\").filter(test_predictions.topic.like(\"wheat%\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve - training: 0.9645390070921985\n",
      "Area under ROC curve - testing: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "## Predicted lables seem accurate on first glance. \n",
    "# Quantify the accuracy.\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\").setMetricName(\"accuracy\")\n",
    "print (\"Area under ROC curve - training:\",evaluator.evaluate(train_predictions))\n",
    "print (\"Area under ROC curve - testing:\",evaluator.evaluate(test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 3 values for idf,\n",
    "# this grid will have 3 x 3 = 9 parameter settings for CrossValidator to choose from.\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(hashingTF.numFeatures,[1000,10000,100000])\\\n",
    "    .addGrid(idf.minDocFreq,[0,10,100])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 640 ms, sys: 196 ms, total: 836 ms\n",
      "Wall time: 2min 19s\n",
      "Area under ROC curve for non-tuned model: 0.9333333333333333\n",
      "Area under ROC curve for best fitted model = 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# A TrainValidationSplit validator method requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setTrainRatio(0.6)\n",
    "\n",
    "# Process time taken to evaluate tvs model.\n",
    "%time tvsModel = tvs.fit(train_set)\n",
    "\n",
    "# Print accuracy results (with and without cross validation).\n",
    "print (\"Area under ROC curve for non-tuned model:\",evaluator.evaluate(test_predictions))\n",
    "print (\"Area under ROC curve for best fitted model =\",evaluator.evaluate(tvsModel.transform(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

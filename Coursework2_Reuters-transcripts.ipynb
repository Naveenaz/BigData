{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed\n--2018-04-22 18:13:59--  https://kdd.ics.uci.edu/databases/reuters_transcribed/ReutersTranscribedSubset.zip\nResolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\nConnecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 148545 (145K) [application/zip]\nSaving to: \u2018ReutersTranscribedSubset.zip\u2019\n\n100%[======================================>] 148,545     --.-K/s   in 0.06s   \n\n2018-04-22 18:13:59 (2.31 MB/s) - \u2018ReutersTranscribedSubset.zip\u2019 saved [148545/148545]\n\nReutersTranscribedSubset.zip\n"
                }
            ], 
            "source": "%rm -rf ~/notebook/work/City-Data-Science/datasets/ReutersTranscribed\n%mkdir ~/notebook/work/City-Data-Science/datasets/ReutersTranscribed\n%cd ~/notebook/work/City-Data-Science/datasets/ReutersTranscribed\n!wget https://kdd.ics.uci.edu/databases/reuters_transcribed/ReutersTranscribedSubset.zip\n!ls"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Archive:  ReutersTranscribedSubset.zip\n   creating: transcriptions/acq/\n  inflating: transcriptions/acq/14843  \n  inflating: transcriptions/acq/14852  \n  inflating: transcriptions/acq/14865  \n  inflating: transcriptions/acq/14888  \n  inflating: transcriptions/acq/14900  \n  inflating: transcriptions/acq/14907  \n  inflating: transcriptions/acq/14909  \n  inflating: transcriptions/acq/14921  \n  inflating: transcriptions/acq/14932  \n  inflating: transcriptions/acq/14941  \n  inflating: transcriptions/acq/14943  \n  inflating: transcriptions/acq/14949  \n  inflating: transcriptions/acq/14978  \n  inflating: transcriptions/acq/14982  \n  inflating: transcriptions/acq/15001  \n  inflating: transcriptions/acq/15004  \n  inflating: transcriptions/acq/15024  \n  inflating: transcriptions/acq/15031  \n  inflating: transcriptions/acq/15037  \n  inflating: transcriptions/acq/15052  \n   creating: transcriptions/corn/\n  inflating: transcriptions/corn/14832  \n  inflating: transcriptions/corn/14858  \n  inflating: transcriptions/corn/15033  \n  inflating: transcriptions/corn/15043  \n  inflating: transcriptions/corn/15106  \n  inflating: transcriptions/corn/15287  \n  inflating: transcriptions/corn/15341  \n  inflating: transcriptions/corn/15618  \n  inflating: transcriptions/corn/15648  \n  inflating: transcriptions/corn/15676  \n  inflating: transcriptions/corn/15686  \n  inflating: transcriptions/corn/15720  \n  inflating: transcriptions/corn/15845  \n  inflating: transcriptions/corn/15856  \n  inflating: transcriptions/corn/15860  \n  inflating: transcriptions/corn/15863  \n  inflating: transcriptions/corn/15871  \n  inflating: transcriptions/corn/15875  \n  inflating: transcriptions/corn/15877  \n  inflating: transcriptions/corn/15890  \n   creating: transcriptions/crude/\n  inflating: transcriptions/crude/14829  \n  inflating: transcriptions/crude/15063  \n  inflating: transcriptions/crude/15200  \n  inflating: transcriptions/crude/15230  \n  inflating: transcriptions/crude/15238  \n  inflating: transcriptions/crude/15244  \n  inflating: transcriptions/crude/15322  \n  inflating: transcriptions/crude/15339  \n  inflating: transcriptions/crude/15344  \n  inflating: transcriptions/crude/15351  \n  inflating: transcriptions/crude/15520  \n  inflating: transcriptions/crude/15939  \n  inflating: transcriptions/crude/15964  \n  inflating: transcriptions/crude/16005  \n  inflating: transcriptions/crude/16007  \n  inflating: transcriptions/crude/16040  \n  inflating: transcriptions/crude/16077  \n  inflating: transcriptions/crude/16366  \n  inflating: transcriptions/crude/16429  \n  inflating: transcriptions/crude/16438  \n   creating: transcriptions/earn/\n  inflating: transcriptions/earn/14859.txt  \n  inflating: transcriptions/earn/14860.txt  \n  inflating: transcriptions/earn/14872.txt  \n  inflating: transcriptions/earn/14873.txt  \n  inflating: transcriptions/earn/14875.txt  \n  inflating: transcriptions/earn/14876.txt  \n  inflating: transcriptions/earn/14899.txt  \n  inflating: transcriptions/earn/14903.txt  \n  inflating: transcriptions/earn/14911.txt  \n  inflating: transcriptions/earn/14926.txt  \n  inflating: transcriptions/earn/14930.txt  \n  inflating: transcriptions/earn/14933.txt  \n  inflating: transcriptions/earn/14934.txt  \n  inflating: transcriptions/earn/14954.txt  \n  inflating: transcriptions/earn/14958.txt  \n  inflating: transcriptions/earn/14960.txt  \n  inflating: transcriptions/earn/14962.txt  \n  inflating: transcriptions/earn/14963.txt  \n  inflating: transcriptions/earn/14965.txt  \n  inflating: transcriptions/earn/14967.txt  \n   creating: transcriptions/grain/\n  inflating: transcriptions/grain/14828.txt  \n  inflating: transcriptions/grain/14832.txt  \n  inflating: transcriptions/grain/14841.txt  \n  inflating: transcriptions/grain/14858.txt  \n  inflating: transcriptions/grain/15033.txt  \n  inflating: transcriptions/grain/15043.txt  \n  inflating: transcriptions/grain/15097.txt  \n  inflating: transcriptions/grain/15106.txt  \n  inflating: transcriptions/grain/15132.txt  \n  inflating: transcriptions/grain/15271.txt  \n  inflating: transcriptions/grain/15273.txt  \n  inflating: transcriptions/grain/15287.txt  \n  inflating: transcriptions/grain/15303.txt  \n  inflating: transcriptions/grain/15341.txt  \n  inflating: transcriptions/grain/15367.txt  \n  inflating: transcriptions/grain/15388.txt  \n  inflating: transcriptions/grain/15472.txt  \n  inflating: transcriptions/grain/15500.txt  \n  inflating: transcriptions/grain/15531.txt  \n  inflating: transcriptions/grain/15567.txt  \n   creating: transcriptions/interest/\n  inflating: transcriptions/interest/14849.txt  \n  inflating: transcriptions/interest/14861.txt  \n  inflating: transcriptions/interest/14890.txt  \n  inflating: transcriptions/interest/14919.txt  \n  inflating: transcriptions/interest/14964.txt  \n  inflating: transcriptions/interest/15049.txt  \n  inflating: transcriptions/interest/15092.txt  \n  inflating: transcriptions/interest/15096.txt  \n  inflating: transcriptions/interest/15212.txt  \n  inflating: transcriptions/interest/15310.txt  \n  inflating: transcriptions/interest/15364.txt  \n  inflating: transcriptions/interest/15378.txt  \n  inflating: transcriptions/interest/15384.txt  \n  inflating: transcriptions/interest/15410.txt  \n  inflating: transcriptions/interest/15436.txt  \n  inflating: transcriptions/interest/15444.txt  \n  inflating: transcriptions/interest/15522.txt  \n  inflating: transcriptions/interest/15539.txt  \n  inflating: transcriptions/interest/15550.txt  \n  inflating: transcriptions/interest/15552.txt  \n   creating: transcriptions/money/\n  inflating: transcriptions/money/14849  \n  inflating: transcriptions/money/14861  \n  inflating: transcriptions/money/14890  \n  inflating: transcriptions/money/14913  \n  inflating: transcriptions/money/14919  \n  inflating: transcriptions/money/14931  \n  inflating: transcriptions/money/14964  \n  inflating: transcriptions/money/14987  \n  inflating: transcriptions/money/15048  \n  inflating: transcriptions/money/15212  \n  inflating: transcriptions/money/15234  \n  inflating: transcriptions/money/15253  \n  inflating: transcriptions/money/15364  \n  inflating: transcriptions/money/15375  \n  inflating: transcriptions/money/15378  \n  inflating: transcriptions/money/15431  \n  inflating: transcriptions/money/15436  \n  inflating: transcriptions/money/15442  \n  inflating: transcriptions/money/15444  \n  inflating: transcriptions/money/15448  \n   creating: transcriptions/ship/\n  inflating: transcriptions/ship/14839.txt  \n  inflating: transcriptions/ship/14957.txt  \n  inflating: transcriptions/ship/14959.txt  \n  inflating: transcriptions/ship/15484.txt  \n  inflating: transcriptions/ship/15531.txt  \n  inflating: transcriptions/ship/15696.txt  \n  inflating: transcriptions/ship/15710.txt  \n  inflating: transcriptions/ship/15726.txt  \n  inflating: transcriptions/ship/15727.txt  \n  inflating: transcriptions/ship/15728.txt  \n  inflating: transcriptions/ship/15942.txt  \n  inflating: transcriptions/ship/16014.txt  \n  inflating: transcriptions/ship/16040.txt  \n  inflating: transcriptions/ship/16076.txt  \n  inflating: transcriptions/ship/16366.txt  \n  inflating: transcriptions/ship/16934.txt  \n  inflating: transcriptions/ship/17055.txt  \n  inflating: transcriptions/ship/17192.txt  \n  inflating: transcriptions/ship/17436.txt  \n  inflating: transcriptions/ship/17462.txt  \n   creating: transcriptions/trade/\n  inflating: transcriptions/trade/14826.txt  \n  inflating: transcriptions/trade/14832.txt  \n  inflating: transcriptions/trade/14858.txt  \n  inflating: transcriptions/trade/14862.txt  \n  inflating: transcriptions/trade/14881.txt  \n  inflating: transcriptions/trade/14904.txt  \n  inflating: transcriptions/trade/14912.txt  \n  inflating: transcriptions/trade/15154.txt  \n  inflating: transcriptions/trade/15171.txt  \n  inflating: transcriptions/trade/15223.txt  \n  inflating: transcriptions/trade/15262.txt  \n  inflating: transcriptions/trade/15313.txt  \n  inflating: transcriptions/trade/15352.txt  \n  inflating: transcriptions/trade/15372.txt  \n  inflating: transcriptions/trade/15375.txt  \n  inflating: transcriptions/trade/15386.txt  \n  inflating: transcriptions/trade/15428.txt  \n  inflating: transcriptions/trade/15430.txt  \n  inflating: transcriptions/trade/15447.txt  \n  inflating: transcriptions/trade/15452.txt  \n  inflating: transcriptions/trade/15552.txt  \n   creating: transcriptions/wheat/\n  inflating: transcriptions/wheat/14841.txt  \n  inflating: transcriptions/wheat/15043.txt  \n  inflating: transcriptions/wheat/15097.txt  \n  inflating: transcriptions/wheat/15132.txt  \n  inflating: transcriptions/wheat/15271.txt  \n  inflating: transcriptions/wheat/15273.txt  \n  inflating: transcriptions/wheat/15341.txt  \n  inflating: transcriptions/wheat/15388.txt  \n  inflating: transcriptions/wheat/15472.txt  \n  inflating: transcriptions/wheat/15500.txt  \n  inflating: transcriptions/wheat/15567.txt  \n  inflating: transcriptions/wheat/15572.txt  \n  inflating: transcriptions/wheat/15582.txt  \n  inflating: transcriptions/wheat/15618.txt  \n  inflating: transcriptions/wheat/15676.txt  \n  inflating: transcriptions/wheat/15686.txt  \n  inflating: transcriptions/wheat/15728.txt  \n  inflating: transcriptions/wheat/15836.txt  \n  inflating: transcriptions/wheat/15845.txt  \n  inflating: transcriptions/wheat/15853.txt  \nReutersTranscribedSubset.zip  transcriptions\n>>> Unzipping finished.\n"
                }
            ], 
            "source": "!unzip ReutersTranscribedSubset.zip\n# '!' calls a program on the machine (the DSX service runs on virtual Linux machines).\n!ls\nprint(\">>> Unzipping finished.\")"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions\n\u001b[0m\u001b[34;42macq\u001b[0m/  \u001b[34;42mcorn\u001b[0m/  \u001b[34;42mcrude\u001b[0m/  \u001b[34;42mearn\u001b[0m/  \u001b[34;42mgrain\u001b[0m/  \u001b[34;42minterest\u001b[0m/  \u001b[34;42mmoney\u001b[0m/  \u001b[34;42mship\u001b[0m/  \u001b[34;42mtrade\u001b[0m/  \u001b[34;42mwheat\u001b[0m/\r\n"
                }
            ], 
            "source": "%cd ~/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions/\n%ls # and show its content"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "p:  /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions/crude\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions/money\n"
                }, 
                {
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling o598.partitions.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions/crude\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:55)\n\tat org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-30-7ffa83845d29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#Create a union of the 2 RDD's so we hava a full set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcombined_RDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrude_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoney_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#print the total number of documents here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    578\u001b[0m                       self.ctx.serializer)\n\u001b[1;32m    579\u001b[0m         if (self.partitioner == other.partitioner and\n\u001b[0;32m--> 580\u001b[0;31m                 self.getNumPartitions() == rdd.getNumPartitions()):\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o598.partitions.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sa9d-6c0d2444cb566f-b96494a81963/notebook/work/City-Data-Science/datasets/ReutersTranscribed/transcriptions/crude\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.spark.input.WholeTextFileInputFormat.setMinPartitions(WholeTextFileInputFormat.scala:55)\n\tat org.apache.spark.rdd.WholeTextFileRDD.getPartitions(WholeTextFileRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:507)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:785)\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "from pyspark.ml import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.param import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.sql import *\n\nfrom pyspark.sql.types import Row\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\nspark = SparkSession.builder.getOrCreate()\n\nimport os.path\n\np = os.path.abspath('./transcriptions/')\nprint(\"p: \", p) # we need the absolute path, as the executors that will read the from directories, will not run in the same environment, so that %cd calls don't help\n\n#here we are setting the path to select 2 topics\ndirPath1 = p + '/crude'\ndirPath2 = p + '/money'\n\nprint(dirPath1)\nprint(dirPath2)\n# Use wholeTextFiles to read both the files\ncrude_rdd = sc.wholeTextFiles(dirPath1)\nmoney_rdd = sc.wholeTextFiles(dirPath2)\n\n\n#print(crude_rdd.take(1))\n#print(money_rdd.take(1))\n\n#Create a union of the 2 RDD's so we hava a full set\ncombined_RDD = crude_rdd.union(money_rdd)\n\n#print the total number of documents here:\nprint ('Number of documents read is:',crude_rdd.count())\nprint ('Number of documents read is:',money_rdd.count())\nprint ('Number of documents read is:',combined_RDD.count())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}